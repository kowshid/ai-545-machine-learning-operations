{
 "cells": [
  {
   "cell_type": "code",
   "id": "fb178055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:59:39.012903Z",
     "start_time": "2026-02-04T20:59:37.164151Z"
    }
   },
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(\"Hyperparameter Tuning Experiment\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/Users/kowshid/Documents/Repos/grad/ai-545-machine-learning-operations/classworks/mlflow-deep-dive-1/mlruns/3', creation_time=1770238778978, experiment_id='3', last_update_time=1770238778978, lifecycle_stage='active', name='Hyperparameter Tuning Experiment', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5861b367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:59:41.631719Z",
     "start_time": "2026-02-04T20:59:39.014245Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "1ce44a32",
   "metadata": {},
   "source": [
    "### Optuna is a framework for automatically finding good hyperparameters by running many smart experiments. You first define an objective function, which trains a model and returns a single score, such as validation error. Each time Optuna calls this function, it creates a trial, meaning one concrete set of hyperparameters and one model training attempt.\n",
    "\n",
    "### A collection of trials forms a study, which represents the full tuning process. Optuna does not try parameters randomly; it uses past trial results to decide which values to try next, gradually focusing on better regions of the search space. Poor trials can be stopped early using pruning, saving computation\n",
    "\n",
    "### Study is the whole optimization process. inside that study, you are then creating many trials.Each of these trails is a single mlflow run with aa trail number in its name. We are doing 30 of these child runs/trials."
   ]
  },
  {
   "cell_type": "code",
   "id": "aee7fa72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:59:41.707049Z",
     "start_time": "2026-02-04T20:59:41.655227Z"
    }
   },
   "source": [
    "import mlflow\n",
    "import optuna\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Setting nested=True will create a child run under the parent run.\n",
    "    with mlflow.start_run(nested=True, run_name=f\"trial_{trial.number}\") as child_run:\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32)\n",
    "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 50, 300, step=10)\n",
    "        rf_max_features = trial.suggest_float(\"rf_max_features\", 0.2, 1.0)\n",
    "        params = {\n",
    "            \"max_depth\": rf_max_depth,\n",
    "            \"n_estimators\": rf_n_estimators,\n",
    "            \"max_features\": rf_max_features,\n",
    "        }\n",
    "        # Log current trial's parameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        regressor_obj = sklearn.ensemble.RandomForestRegressor(**params)\n",
    "        regressor_obj.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = regressor_obj.predict(X_val)\n",
    "        error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n",
    "        # Log current trial's error metric\n",
    "        mlflow.log_metrics({\"error\": error})\n",
    "\n",
    "        # Log the model file\n",
    "        mlflow.sklearn.log_model(regressor_obj, name=\"model\")\n",
    "        # Make it easy to retrieve the best-performing child run later\n",
    "        trial.set_user_attr(\"run_id\", child_run.info.run_id)\n",
    "        return error"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c76533d0",
   "metadata": {},
   "source": [
    "### Create a study called minimize (Optuna concept, not mlflow). Its a minimize direction cause we are trying to minimize the validation loss. Set how many trials/child runs you want it to have.use mlflow logs accrodingly to log whatever you want ie metrics, params etc. or use aulog to log everything."
   ]
  },
  {
   "cell_type": "code",
   "id": "42f64633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:04:11.809710Z",
     "start_time": "2026-02-04T20:59:41.721970Z"
    }
   },
   "source": [
    "# Create a parent run that contains all child runs for different trials\n",
    "with mlflow.start_run(run_name=\"study\") as run:\n",
    "    # Log the experiment settings\n",
    "    n_trials = 30\n",
    "    mlflow.log_param(\"n_trials\", n_trials)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Log the best trial and its run ID\n",
    "    mlflow.log_params(study.best_trial.params)\n",
    "    mlflow.log_metrics({\"best_error\": study.best_value})\n",
    "    if best_run_id := study.best_trial.user_attrs.get(\"run_id\"):\n",
    "        mlflow.log_param(\"best_child_run_id\", best_run_id)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-04 16:04:11,791]\u001B[0m Trial 29 finished with value: 0.2957123823778443 and parameters: {'rf_max_depth': 16, 'rf_n_estimators': 250, 'rf_max_features': 0.21674326379642894}. Best is trial 17 with value: 0.24456309456152023.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "27eb8011",
   "metadata": {},
   "source": [
    "### After the whole tuning is over, you can click on those trials, sort them by the validation error (or whatever objective function you have), and then register it to the model register! Then you shoulkd be able to see this registered model the model tab. You can add some metadata to that model as well ie which dataset it worked on, who created it etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "abea21df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:07:02.507291Z",
     "start_time": "2026-02-04T21:07:02.398918Z"
    }
   },
   "source": [
    "mlflow.register_model(\n",
    "    model_uri=\"runs:/2ab4afd0c2d849fb82b182896a2b0aaa/model\",\n",
    "    name=\"housing-price-predictor\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1770239222461, current_stage='None', deployment_job_state=None, description=None, last_updated_timestamp=1770239222461, metrics=None, model_id=None, name='housing-price-predictor', params=None, run_id='2ab4afd0c2d849fb82b182896a2b0aaa', run_link=None, source='models:/m-7fcfa63d0d86499caeda6a0e4fb9eaad', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
