{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ecf9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75f03ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4bc64",
   "metadata": {},
   "source": [
    "#### in a models array, define different models wih their different respective parameter values and the train and test dataset to get heir performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d2dece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression_liblinear\",\n",
    "        {\"C\": 1, \"solver\": 'liblinear'},\n",
    "        LogisticRegression(),\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest_30\",\n",
    "        {\"n_estimators\": 30, \"max_depth\": 3},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Logistic Regression_lbfgs\",\n",
    "        {\"max_iter\": 100, \"solver\": 'lbfgs'},\n",
    "        LogisticRegression(),\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest_100\",\n",
    "        {\"n_estimators\": 100, \"max_depth\": 7},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec7581",
   "metadata": {},
   "source": [
    "#### Loop through the models array and the the classification report for each of those models. The classification report reports detailed precision and recall for individual classes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4364c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "for model_name, params, model, train_set, test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04552a9",
   "metadata": {},
   "source": [
    "#### Set a new experiment in mlflow and then log the relevant hings such as model name, parameters and accuracy, recall for the individual classes and f1-score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5d9fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Model Management Quickstart\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "    report = reports[i]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\n",
    "            'accuracy': report['accuracy'],\n",
    "            'recall_class_1': report['1']['recall'],\n",
    "            'recall_class_0': report['0']['recall'],\n",
    "            'f1_score_macro': report['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "        mlflow.sklearn.log_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd7813",
   "metadata": {},
   "source": [
    "#### Once all the models are logged, programatically load the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd6c931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = mlflow.search_logged_models(\n",
    "    experiment_ids=[\"3\"],\n",
    "    filter_string=\"metrics.accuracy> 0.97\",\n",
    "    order_by=[{\"field_name\": \"metrics.accuracy\", \"ascending\": False}],\n",
    "    max_results=3,\n",
    ")\n",
    "\n",
    "best_model = top_models.iloc[0]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213d78b",
   "metadata": {},
   "source": [
    "#### Register the best performing model and name is \"best iris model\". Once that is done, you should be able to see this model on MLFlow-->Models as registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df5961c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered = mlflow.register_model(\n",
    "    model_uri=f\"models:/{best_model.model_id}\",\n",
    "    name=\"best_iris_model\",\n",
    ")\n",
    "print(\"registered version:\", registered.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ba47c",
   "metadata": {},
   "source": [
    "Add tags and aliases to this registered model. Champion means this model is the one deployed. Challenger means these are models tha are caandidates to replace the champion. Once these aliases are defined and given, loading them gets even easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0bc9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "\n",
    "client.update_model_version(\n",
    "    name=\"best_iris_model\",\n",
    "    version=registered.version,\n",
    "    description=(\n",
    "        \"Champion model selected via mlflow.search_logged_models, \"\n",
    "        f\"run_id={best_model.source_run_id}, metric=accuracy.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=\"best_iris_model\",\n",
    "    version=registered.version,\n",
    "    key=\"role\",\n",
    "    value=\"champion\",\n",
    ")\n",
    "client.set_model_version_tag(\n",
    "    name=\"best_iris_model\",\n",
    "    version=registered.version,\n",
    "    key=\"source_experiment_id\",\n",
    "\n",
    "    value=best_model.experiment_id,\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=\"best_iris_model\",\n",
    "    version=registered.version,\n",
    "    key=\"run_id\",\n",
    "    value=best_model.source_run_id,\n",
    ")\n",
    "\n",
    "# 4. Assign an alias (e.g., champion) to this version\n",
    "client.set_registered_model_alias(\n",
    "    name=\"best_iris_model\",\n",
    "    alias=\"champion\",\n",
    "    version=registered.version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4955685",
   "metadata": {},
   "source": [
    "#### You can just load the champion model for the best iris model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53d2da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champion_model = mlflow.sklearn.load_model(\n",
    "#     model_uri=\"models:/best_iris_model@champion\"\n",
    "# )\n",
    "# print(champion_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f3ab3",
   "metadata": {},
   "source": [
    "#### Sanity check: if the loaded champion model was indeed the top model thata we found before by checking the corresponding run ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b19a1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champion_model = client.get_model_version_by_alias(\n",
    "#     name=\"best_iris_model\",\n",
    "#     alias=\"champion\"\n",
    "# )\n",
    "# print(champion_model.run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
