{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37bdd61",
   "metadata": {},
   "source": [
    "### Enabling system metrics logging makes MLflow track CPU usage, memory consumption, disk I/O, and sometimes GPU stats while a run is executing. Setting the sampling interval to one second means these metrics are captured every second"
   ]
  },
  {
   "cell_type": "code",
   "id": "ecd946e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:11:16.847117Z",
     "start_time": "2026-02-04T21:11:16.790401Z"
    }
   },
   "source": [
    "import mlflow\n",
    "\n",
    "# The set_experiment API creates a new experiment if it doesn't exist.\n",
    "mlflow.set_experiment(\"Deep Learning Experiment\")\n",
    "\n",
    "# IMPORTANT: Enable system metrics monitoring\n",
    "mlflow.config.enable_system_metrics_logging()\n",
    "mlflow.config.set_system_metrics_sampling_interval(1)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "31cc008e",
   "metadata": {},
   "source": [
    "### import the packages accordingly. Select the device. \n",
    "### z-score normalize the data using FashionMNIST dataset's precomputed mean and std dev values\n",
    "### train dataset load, train is true to let torch know we will do training on these samples. test dataset train HAS to be false\n",
    "### train loader to load 64 training samples during training,make it shuffling for each epoch to reduce memorization\n",
    "### for test, we just are doing inference, so load 1000 maybe"
   ]
  },
  {
   "cell_type": "code",
   "id": "a945b8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:11:16.900298Z",
     "start_time": "2026-02-04T21:11:16.854308Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and prepare data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.FashionMNIST(\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(\"data\", train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "1271f968",
   "metadata": {},
   "source": [
    "### Defining a Neural NEtwork. First, take the 28*28 2D images and flatten them into a 1D 28*28=784 length flattened tensor(fancy array basically)\n",
    "\n",
    "### then first neural layer, 784 to 512 with ReLu\n",
    "\n",
    "### then second neural layer, 512 to 512 with ReLu\n",
    "\n",
    "### final layer 512 from previous to 10 cause we are predicting 10 types of dress\n",
    "\n",
    "### The forward method defines how data flows through the model. It flattens the input, passes it through the stack of layers, and returns raw outputs called logits. These logits are later converted into probabilities by a loss function such as CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "id": "47c05ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:11:16.917548Z",
     "start_time": "2026-02-04T21:11:16.901060Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "4ff0d8ce",
   "metadata": {},
   "source": [
    "hyperparameters and loading the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "id": "9772ce35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:11:16.947628Z",
     "start_time": "2026-02-04T21:11:16.918669Z"
    }
   },
   "source": [
    "# Training parameters\n",
    "params = {\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"model_type\": \"MLP\",\n",
    "    \"hidden_units\": [512, 512],\n",
    "}\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0165989c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:11:56.461527Z",
     "start_time": "2026-02-04T21:11:16.951389Z"
    }
   },
   "source": [
    "with mlflow.start_run() as run:\n",
    "    # Log training parameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Log batch metrics (every 100 batches)\n",
    "            if batch_idx % 100 == 0:\n",
    "                batch_loss = train_loss / (batch_idx + 1)\n",
    "                batch_acc = 100.0 * correct / total\n",
    "                mlflow.log_metrics(\n",
    "                    {\"batch_loss\": batch_loss, \"batch_accuracy\": batch_acc},\n",
    "                    step=epoch * len(train_loader) + batch_idx,\n",
    "                )\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        # Calculate and log epoch validation metrics\n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Log epoch metrics\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"train_accuracy\": epoch_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "            },\n",
    "            step=epoch,\n",
    "        )\n",
    "        # Log checkpoint at the end of each epoch\n",
    "        mlflow.pytorch.log_model(model, name=f\"checkpoint_{epoch}\")\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{params['epochs']}, \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # Log the final trained model\n",
    "    model_info = mlflow.pytorch.log_model(model, name=\"final_model\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/04 16:11:56 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "c4acf256",
   "metadata": {},
   "source": [
    "### no need to calculate gradient again"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9778cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T21:15:19.426521Z",
     "start_time": "2026-02-04T21:15:18.831630Z"
    }
   },
   "source": [
    "# Load the final model\n",
    "model = mlflow.pytorch.load_model(\"runs:/73fee1e979f945aeb740fea1fbc3bb7b/final_model\")\n",
    "# or load a checkpoint\n",
    "# model = mlflow.pytorch.load_model(\"runs:/<run_id>/checkpoint_<epoch>\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Resume the previous run to log test metrics\n",
    "with mlflow.start_run(run_id=run.info.run_id) as run:\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_correct, test_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        test_total += target.size(0)\n",
    "        test_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    # Calculate and log final test metrics\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "\n",
    "    mlflow.log_metrics({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
    "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 77.30%\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
